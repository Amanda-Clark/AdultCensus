{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engr(train_set, test_set):\n",
    "    '''\n",
    "    Taking the raw training and test sets, and cleaning them\n",
    "    up so that they can be used in training a model\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    train_set: pandas dataframe\n",
    "        training set read in from pandas\n",
    "    test_set: pandas dataframe\n",
    "        test set read in from pandas\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    train_scaled, test_scaled, y_train, y_test\n",
    "        feature engineered and cleaned up sets   \n",
    "    '''\n",
    "    train_set =train_set.dropna()\n",
    "    test_set = test_set.dropna()\n",
    "    \n",
    "    combined = pd.concat([train_set, test_set], axis=0).reset_index(drop=True)\n",
    "    \n",
    "    combined.loc[combined.target == ' >50K', 'target']=1\n",
    "    combined.loc[combined.target==' <=50K', 'target']=0\n",
    "    combined.loc[combined.target==' <=50K.', 'target']=0\n",
    "    combined.loc[combined.target == ' >50K.', 'target']=1\n",
    "    \n",
    "       \n",
    "    dummies = pd.get_dummies(combined)\n",
    "    \n",
    "    ts = len(train_set)\n",
    "    ts2 = len(test_set)\n",
    "    \n",
    "    train_set = dummies[0:ts]\n",
    "    test_set = dummies[ts:]\n",
    "    \n",
    "    y_train = train_set['target']\n",
    "    y_test = test_set['target']\n",
    "    \n",
    "    train_set = train_set.drop([\"target\"], axis=1)\n",
    "    test_set = test_set.drop([\"target\"], axis=1)\n",
    "    \n",
    "    std_scaler = StandardScaler()\n",
    "    std_scaler.fit(train_set)\n",
    "    train_scaled = std_scaler.transform(train_set)\n",
    "    test_scaled = std_scaler.transform(test_set)\n",
    "    \n",
    "\n",
    "    return train_scaled, test_scaled, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=['age', 'workclass','fnlwgt','education','education-num','marital-status','occupation',\n",
    "         'relationship','race','sex','capital-gain','capital-loss','hours-per-week','native-country', 'target']\n",
    "test_set = pd.read_csv(\"adult.test.txt\", index_col=False, names=columns, na_values=[' ?'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=['age', 'workclass','fnlwgt','education','education-num','marital-status','occupation',\n",
    "         'relationship','race','sex','capital-gain','capital-loss','hours-per-week','native-country', 'target']\n",
    "train_set = pd.read_csv(\"adult.data.txt\", index_col=False, names=columns,na_values=[' ?'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype uint8, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:45: DataConversionWarning: Data with input dtype uint8, float64 were all converted to float64 by StandardScaler.\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:46: DataConversionWarning: Data with input dtype uint8, float64 were all converted to float64 by StandardScaler.\n"
     ]
    }
   ],
   "source": [
    "train_scaled, test_scaled, y_train, y_test = feature_engr(train_set, test_set)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = train_scaled.shape\n",
    "#num_classes = 2\n",
    "batch_size = 128\n",
    "epochs = 20\n",
    "#y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "#y_test = keras.utils.to_categorical(y_test, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    '''\n",
    "    Building a basic sequential model, with me just guessing basically\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    compiled neural net\n",
    "    '''\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, activation='elu', input_shape=(dim[1],)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(256, activation='elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(256, activation='elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(256, activation='elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(128, activation='elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(64, activation='elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "    optimizer=Adam(),\n",
    "    metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_73 (Dense)             (None, 512)               127488    \n",
      "_________________________________________________________________\n",
      "batch_normalization_31 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_59 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_74 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_32 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_60 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_75 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_33 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_61 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_76 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_34 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_62 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_77 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_35 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_63 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_78 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_36 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_64 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_79 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 437,505\n",
      "Trainable params: 434,561\n",
      "Non-trainable params: 2,944\n",
      "_________________________________________________________________\n",
      "Train on 25637 samples, validate on 4525 samples\n",
      "Epoch 1/20\n",
      "25637/25637 [==============================] - 9s 333us/step - loss: 0.4665 - acc: 0.7890 - val_loss: 0.3707 - val_acc: 0.8331\n",
      "Epoch 2/20\n",
      "25637/25637 [==============================] - 5s 211us/step - loss: 0.3663 - acc: 0.8306 - val_loss: 0.3540 - val_acc: 0.8393\n",
      "Epoch 3/20\n",
      "25637/25637 [==============================] - 6s 221us/step - loss: 0.3466 - acc: 0.8371 - val_loss: 0.3342 - val_acc: 0.8455\n",
      "Epoch 4/20\n",
      "25637/25637 [==============================] - 6s 221us/step - loss: 0.3366 - acc: 0.8411 - val_loss: 0.3280 - val_acc: 0.8510\n",
      "Epoch 5/20\n",
      "25637/25637 [==============================] - 6s 215us/step - loss: 0.3321 - acc: 0.8422 - val_loss: 0.3248 - val_acc: 0.8508\n",
      "Epoch 6/20\n",
      "25637/25637 [==============================] - 5s 198us/step - loss: 0.3293 - acc: 0.8460 - val_loss: 0.3225 - val_acc: 0.8486\n",
      "Epoch 7/20\n",
      "25637/25637 [==============================] - 5s 196us/step - loss: 0.3258 - acc: 0.8468 - val_loss: 0.3229 - val_acc: 0.8493\n",
      "Epoch 8/20\n",
      "25637/25637 [==============================] - 5s 194us/step - loss: 0.3214 - acc: 0.8477 - val_loss: 0.3297 - val_acc: 0.8466\n",
      "Epoch 9/20\n",
      "25637/25637 [==============================] - 5s 200us/step - loss: 0.3148 - acc: 0.8556 - val_loss: 0.3280 - val_acc: 0.8486\n",
      "Epoch 10/20\n",
      "25637/25637 [==============================] - 5s 206us/step - loss: 0.3150 - acc: 0.8546 - val_loss: 0.3277 - val_acc: 0.8486\n",
      "Epoch 11/20\n",
      "25637/25637 [==============================] - 5s 203us/step - loss: 0.3124 - acc: 0.8532 - val_loss: 0.3221 - val_acc: 0.8488\n",
      "Epoch 12/20\n",
      "25637/25637 [==============================] - 5s 200us/step - loss: 0.3079 - acc: 0.8562 - val_loss: 0.3301 - val_acc: 0.8469\n",
      "Epoch 13/20\n",
      "25637/25637 [==============================] - 5s 211us/step - loss: 0.3049 - acc: 0.8573 - val_loss: 0.3285 - val_acc: 0.8473\n",
      "Epoch 14/20\n",
      "25637/25637 [==============================] - 5s 203us/step - loss: 0.3003 - acc: 0.8592 - val_loss: 0.3303 - val_acc: 0.8440\n",
      "Epoch 15/20\n",
      "25637/25637 [==============================] - 6s 215us/step - loss: 0.3001 - acc: 0.8585 - val_loss: 0.3331 - val_acc: 0.8435\n",
      "Epoch 16/20\n",
      "25637/25637 [==============================] - 6s 246us/step - loss: 0.2978 - acc: 0.8601 - val_loss: 0.3274 - val_acc: 0.8497\n",
      "Epoch 17/20\n",
      "25637/25637 [==============================] - 6s 236us/step - loss: 0.2934 - acc: 0.8625 - val_loss: 0.3348 - val_acc: 0.8469\n",
      "Epoch 18/20\n",
      "25637/25637 [==============================] - 5s 199us/step - loss: 0.2892 - acc: 0.8628 - val_loss: 0.3446 - val_acc: 0.8455\n",
      "Epoch 19/20\n",
      "25637/25637 [==============================] - 5s 199us/step - loss: 0.2874 - acc: 0.8639 - val_loss: 0.3454 - val_acc: 0.8413\n",
      "Epoch 20/20\n",
      "25637/25637 [==============================] - 5s 204us/step - loss: 0.2836 - acc: 0.8662 - val_loss: 0.3432 - val_acc: 0.8466\n"
     ]
    }
   ],
   "source": [
    "mod = build_model()\n",
    "history = mod.fit(train_scaled, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.36551714549818204\n",
      "Test accuracy: 0.837715803437024\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(test_scaled, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
